{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc96eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defdec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,     \n",
    "    gravity=-10.0,        \n",
    "    enable_wind=False,   \n",
    "    wind_power=15.0,      \n",
    "    turbulence_power=1.0, \n",
    "    render_mode=\"rgb_array\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21af3cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "space_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "n_epochs = 1000\n",
    "eval_epochs = 500\n",
    "gamma = 0.99\n",
    "alpha = 0.001\n",
    "alpha_c = 0.002\n",
    "target_steps = 1000\n",
    "entropy_coefficient_start = 0.1\n",
    "entropy_coefficient_end = 0.01\n",
    "entropy_coefficient_decay = 0.995\n",
    "entropy_coefficient = entropy_coefficient_start\n",
    "pbar = tqdm(range(n_epochs))\n",
    "max_ave_steps = 0\n",
    "max_ave_rewards = 0 \n",
    "n_steps = []\n",
    "rewards = []\n",
    "\n",
    "# The network to select an action\n",
    "ActorNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, action_dim)\n",
    ")\n",
    "\n",
    "# The network to get value of a state\n",
    "CriticNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "# Optimizer using Adam Gradient Descent\n",
    "actor_optimizer = optim.Adam(ActorNetwork.parameters(), lr=alpha)\n",
    "critic_optimizer = optim.Adam(CriticNetwork.parameters(), lr=alpha_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e60c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1000 Average steps: 108.02, Highest step: 149.04, Current reward: -216.50400401914166: 100%|██████████| 1000/1000 [03:56<00:00,  4.22it/s]       \n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for t in pbar:\n",
    "    state, _ = env.reset()          # Get s_t state\n",
    "    state = torch.FloatTensor(state)\n",
    "    count = 0\n",
    "    epoch_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done: \n",
    "        action_pred = ActorNetwork(state)                       # Select action a_t\n",
    "        action_prods = F.softmax(action_pred, dim=-1)\n",
    "        dist = distributions.Categorical(action_prods)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        next_state, reward, done, truncated, _ = env.step(action=action.item())        # Get state s_t+1, reward r_t+1 from environment\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        done = done or truncated\n",
    "\n",
    "        advantage = reward + (1 - done) * gamma * (CriticNetwork(next_state) - CriticNetwork(state))   # Calculate Advantage (TD Error)\n",
    "\n",
    "        critic_loss = 0.5 * advantage**2                    # Critic loss using advantage\n",
    "        actor_loss = -log_prob * advantage.detach()         # Actor loss using advantage and log probability of distribution\n",
    "\n",
    "        entropy = dist.entropy()            \n",
    "        actor_loss = actor_loss - entropy_coefficient * entropy        # Entropy regularization trick\n",
    "        \n",
    "        actor_optimizer.zero_grad()         # Update Actor\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        critic_optimizer.zero_grad()        # Update Critic\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        state = next_state                  # Move to next state\n",
    "        epoch_reward += reward              # Get cumulative reward through states\n",
    "        count += 1                          # Count number of steps     \n",
    "\n",
    "    rewards.append(epoch_reward)\n",
    "    n_steps.append(count)\n",
    "\n",
    "    entropy_coefficient = max(entropy_coefficient_end, entropy_coefficient_start * entropy_coefficient_decay)\n",
    "\n",
    "    ave_steps = np.mean(n_steps[-50:])\n",
    "    ave_rewards = np.mean(rewards[-50:])\n",
    "\n",
    "    if ave_steps > max_ave_steps:\n",
    "        max_ave_steps = ave_steps\n",
    "\n",
    "    if ave_rewards > max_ave_rewards:\n",
    "        max_ave_rewards = ave_rewards\n",
    "    \n",
    "    pbar.set_description(f\"Epoch {t+1} Average steps: {ave_steps}, Highest step: {max_ave_steps}, Current reward: {ave_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1d8763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average evaluation reward: -100.21\n"
     ]
    }
   ],
   "source": [
    "# Evaluation in 1000 epochs using highest softmax action and save to a video file    \n",
    "env = RecordVideo(env, \"./a2c_lunar_lander\", episode_trigger=lambda x: x % 100 == 0)\n",
    "total_evaluation_reward = 0\n",
    "state, _ = env.reset()\n",
    "state = torch.FloatTensor(state)\n",
    "for _ in range(eval_epochs):\n",
    "    epoch_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action_pred = ActorNetwork(state)                       # Select action a_t\n",
    "        action_prods = F.softmax(action_pred, dim=-1)\n",
    "        action = torch.argmax(action_prods).item()\n",
    "       \n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        state = torch.FloatTensor(state)\n",
    "        epoch_reward += reward\n",
    "        done = done or truncated\n",
    "    total_evaluation_reward += epoch_reward\n",
    "print(f\"Average evaluation reward: {total_evaluation_reward / eval_epochs:.2f}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
