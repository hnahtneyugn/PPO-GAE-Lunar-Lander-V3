{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b52cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and dependencies\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as distributions\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdf663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training environment (wind disabled)\n",
    "env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,     \n",
    "    gravity=-10.0,        \n",
    "    enable_wind=False,   \n",
    "    wind_power=15.0,      \n",
    "    turbulence_power=1.0, \n",
    "    render_mode=\"rgb_array\" \n",
    ")\n",
    "\n",
    "# Evaluation environment (wind enabled)\n",
    "video_env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,     \n",
    "    gravity=-10.0,        \n",
    "    enable_wind=False,   \n",
    "    wind_power=15.0,      \n",
    "    turbulence_power=1.0, \n",
    "    render_mode=\"rgb_array\" \n",
    ")\n",
    "\n",
    "# Record video every 100 epochs\n",
    "video_env = RecordVideo(video_env, video_folder=\"a2c_1e6_256_2048_3e4\", episode_trigger=lambda x: x % 1000 == 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_dim = env.observation_space.shape[0]      # Observation space: 8-dimensional vector\n",
    "action_dim = env.action_space.n                 # Action space: 4 discrete actions\n",
    "\n",
    "# Training parameters (following paper)\n",
    "gamma = 0.99                # Discount factor\n",
    "lr = 3e-4                   # Learning rate\n",
    "lamb = 0.95                 # Generalised Advantage Estimation (GAE) lambda\n",
    "epsilon = 0.2               # Clipping value\n",
    "h = 0.01                    # Entropy coefficient\n",
    "v = 0.5                     # Value loss coefficient\n",
    "max_timesteps = 1e6         # Maximal number of iterations\n",
    "eval_episodes = 100         # Episodes for evaluation\n",
    "N = 1                       # Number of agents collecting training data\n",
    "T = 2048                    # Maximal trajectory length\n",
    "K = 10                      # Number of epoches per update\n",
    "minibatch_size = 256        # Size of a mini batch\n",
    "number_minibatches = N * T / minibatch_size     # Number of mini batches\n",
    "actor_losses = []           # For plotting\n",
    "critic_losses = []\n",
    "eval_rewards = []            \n",
    "index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a47cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network to select an action\n",
    "ActorNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "\n",
    "# The network to get value of a state\n",
    "CriticNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "# Optimizer using Adam Gradient Descent\n",
    "actor_optimizer = optim.Adam(ActorNetwork.parameters(), lr=lr)\n",
    "critic_optimizer = optim.Adam(CriticNetwork.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecee605",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "timesteps = 0\n",
    "\n",
    "while timesteps < max_timesteps:\n",
    "    states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n",
    "    print(timesteps)\n",
    "\n",
    "    for _ in range(T):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
