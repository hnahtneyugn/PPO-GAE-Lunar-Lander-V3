{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a83192a",
   "metadata": {},
   "source": [
    "# 1. Giới thiệu\n",
    "\n",
    "Reinforcement Learning (RL) hay Học tăng cường là một nhóm các thuật toán Học máy có thể được huấn luyện bằng cách phản ánh lại xem hành vi gần nhất của chúng trong môi trường nhất định là tốt hay xấu. Trong số đó, các phương pháp khác nhau dựa trên Học giá trị hoặc Học chiến lược đều được đánh giá cao vì có hiệu quả nhất định và có nhiều tiềm năng trong các lĩnh vực khác nhau trong cuộc sống. \n",
    "\n",
    "Trong bài tập lớn lần này, em áp dụng một ứng dụng của bài toán Học tăng cường qua môi trường mô phỏng [Lunar Lander - Gymnasium Documentation](https://gymnasium.farama.org/environments/box2d/lunar_lander/). Trong môi trường này, người chơi có trách nhiệm điều khiển tàu vũ trụ hạ cánh an toàn trên bề mặt mặt trăng nơi có những yếu tố bên ngoài tác động như trọng lực, sự nhiễu loạn,....\n",
    "\n",
    "Xuyên suốt quá trình thực hiện bài tập này, em đã tìm hiểu và áp dụng cũng như huấn luyện mô hình dựa trên một số thuật toán khác nhau, có thể kể đến đầu tiên là Advantage Actor Critic (A2C), một mô hình Học tăng cường lai giữa Học giá trị và Học chiến lược giúp ổn định quá trình huấn luyện bằng cách giảm phương sai qua Actor và Critic. Tuy nhiên, mô hình này không có kết quả tốt trong quá trình huấn luyện nên em đã chuyển sang một mô hình có tính ổn định cao hơn là Proximal Policy Optimization (PPO) kết hợp với Generalized Advantage Estimator (GAE). Trong notebook này, em xin phép trình bày về thuật toán, mô hình, cũng như cách thực thi với môi trường Lunar Lander và báo cáo kết quả thu được."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5a7f6",
   "metadata": {},
   "source": [
    "# 2. Thuật toán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f60eb9",
   "metadata": {},
   "source": [
    "## 2.1. Advantage Actor Critic (A2C)\n",
    "\n",
    "Trong Học tăng cường, chúng ta muốn tăng xác suất của hành động trong một quỹ đạo tỉ lệ với điểm thường tích luỹ được: $$\\nabla_{\\theta}J(\\theta) = \\sum_{t=0}\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})R(\\tau)$$\n",
    "Trong đó $\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})$ là hướng tăng mạnh nhất của log xác suất của việc chọn hành động $a_t$ từ trạng thái $s_t$ , còn $R(\\tau)$ là điểm thưởng tích luỹ trên cả quỹ đạo $\\tau$.\n",
    "\n",
    "Nếu điểm tích luỹ cao, chúng ta sẽ tăng xác suất rơi vào cặp trạng thái - hành động đó, và ngược lại.\n",
    "\n",
    "Điểm thưởng tích luỹ $R(\\tau)$ được tính bằng cách lấy mẫu Monte-Carlo: lấy một quỹ đạo và tính điểm thưởng có chiết khấu trên quỹ đạo đó, và dùng giá trị này để tăng / giảm xác suất của mọi hành động trên quỹ đạo này. Nếu điểm thưởng trả về là tốt, vậy mọi hành động trên quỹ đạo đó sẽ được \"tăng cường\" bằng cách tăng xác suất được lựa chọn. $$R(\\tau) = R_{t+1} + \\gamma R_{t+2}+\\gamma^2R_{t+3}+\\dots$$\n",
    "Tuy nhiên, vì môi trường có những yếu tố ngẫu nhiên trong mỗi lần chơi và tính ngẫu nhiên trong từng chiến lược, các quỹ đạo có thể dẫn tới những điểm thưởng khác nhau, dẫn tới tăng phương sai. Nói cách khác, cùng một trạng thái bắt đầu những có thể dẫn tới nhiều kết quả khác nhau trong những lần chơi khác nhau. \n",
    "\n",
    "![](images/image41.png)\n",
    "\n",
    "**Giải pháp**: Giảm bớt phương sai bằng cách dùng số lượng lớn quỹ đạo, với hi vọng rằng phương sai trong bất kì một quỹ đạo nào cũng sẽ được giảm bớt trong tổng thể và qua đó cho ta một ước lượng thật chuẩn xác về kết quả đúng.\n",
    "\n",
    "Tuy rằng giải pháp khả thi, nhưng việc dùng nhiều quỹ đạo cùng lúc làm giảm hiệu quả lấy mẫu. Từ đó, thuật toán A2C ra đời nhằm giải quyết vấn đề về phương sai.\n",
    "\n",
    "Ý tưởng cơ bản: ta có 2 hàm xấp xỉ:\n",
    "- Actor $\\pi_{\\theta}(s)$: Thực hiện hành động ngẫu nhiên.\n",
    "- Critic $V(s)$: Quan sát hành động và phản hồi lại, qua đó Actor sẽ thay đổi chiến lược của mình để chơi tốt hơn.\n",
    "\n",
    "Hàm Advantage: tính toán xem tại một trạng thái nhất định $s$, việc thực hiện hành động $a$ sẽ tốt hơn như thế nào so với điểm thưởng trung bình của tất cả hành động khác tại trạng thái đó $$A(s,a) = Q(s,a) - V(s)$$\n",
    "Ta kí hiệu tham số của Actor là $\\theta$, tham số của Critic là $\\theta_{c}$, learning rate của Actor là $\\alpha$, learning rate của Critic là $\\alpha_{c}$\n",
    "\n",
    "Quá trình thực hiện thuật toán như sau:\n",
    "1. Bắt đầu với trạng thái ban đầu $s_{t}$. Actor lựa chọn một hành động ngẫu nhiên $a_{t}$: $$a_{t} \\sim \\pi(.|s_{t};\\theta)$$\n",
    "2. Môi trường nhận hành động $a_{t}$, trả về điểm thưởng $r_{t+1}$ cho Critic và trạng thái mới $s_{t+1}$ cho cả Critic và Actor.\n",
    "3. Critic đánh giá Temporal Difference (TD Error), đây cũng sẽ là giá trị ước lượng thay cho công thức hàm Advantage ở trên: $$\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1};\\theta_{c}) - V(s_{t};\\theta_{c})$$\n",
    "4. Critic và Actor cập nhật trọng số cho mô hình bằng cách dùng TD Error\n",
    "\t1. Cập nhật cho Critic\n",
    "\t\t1. Hàm loss: $$L_{c}(\\theta_{c}) = \\frac{1}{2}(r_{t+1} + \\gamma V(s_{t+1};\\theta_{c}) - V(s_{t};\\theta_{c}))^2 = \\frac{1}{2}\\delta_{t}^2$$\n",
    "\t\t2. Gradient: $$\\nabla_{\\theta_{c}}L(\\theta_{c}) = \\delta_{t}.(\\gamma \\nabla_{\\theta_{c}}V(s_{t+1};\\theta_{c}) - \\nabla_{\\theta_{c}}V(s_{t};\\theta_{c}))$$\n",
    "\t\t3. Gradient Descent: $$\\theta_{c} = \\theta_{c} - \\alpha_{c}.\\nabla_{\\theta_{c}}L(\\theta_{c})$$\n",
    "\t2. Cập nhật cho Actor\n",
    "\t\t1. Hàm loss: $$L(\\theta) = -\\log \\pi(a_{t}|s_{t};\\theta).\\delta_{t}$$\n",
    "\t\t2. Gradient: $$\\nabla_{\\theta}L(\\theta) = -\\nabla_{\\theta}\\log \\pi(a_{t}|s_{t};\\theta).\\delta_{t}$$\n",
    "\t\t3. Gradient Descent: $$\\theta = \\theta - \\alpha.\\nabla_{\\theta}L(\\theta)$$\n",
    "5. Đi tới trạng thái tiếp theo $s_t = s_{t+1}$, Actor tiếp tục lựa chọn hành động $a_{t+1}$, sau đó thuật toán lặp lại tới khi kết thúc.\n",
    "\n",
    "Một số lưu ý về A2C như sau: \n",
    "- $\\pi(a_{t}|s_{t};\\theta)$ là một mạng nơ-ron để chọn hành động dựa trên trạng thái hiện tại, $V(s)$ là một mạng nơ-ron để lấy giá trị điểm thưởng từ trạng thái $s$.\n",
    "- Với $J(\\theta)$ là ước lượng điểm thưởng tích luỹ: $$\\nabla_{\\theta}J(\\theta) \\approx E[\\nabla_{\\theta}\\log \\pi(a_{t}|s_{t};\\theta).\\delta_{t}]$$, nói cách khác, $\\nabla_{\\theta}J(\\theta) = -\\nabla_{\\theta}L(\\theta)$, việc cực tiểu hoá hàm loss của Actor đồng nghĩa với việc cực đại hoá ước lượng điểm thưởng tích luỹ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00514a",
   "metadata": {},
   "source": [
    "## 2.2. Proximal Policy Optimization (PPO)\n",
    "\n",
    "Tuy A2C có thể giải được các bài toán Học tăng cường đơn giản (Như Lunar Lander) trong một số bước hữu hạn, ở một số trường hợp nhất định, thuật toán tỏ ra không hiệu quả, có thể rơi vào trường hợp thay đổi chiến lược quá nhanh quá mạnh hoặc không thật sự học được gì (qua thực tế huấn luyện). Chính vì thế, em đã thay đổi sang một thuật toán mang tính ổn định cao hơn là Proximal Policy Optimization (PPO), tích hợp thêm Generalised Advantage Estimation (GAE) để ước lượng Advantage. \n",
    "\n",
    "Điểm nổi bật của thuật toán PPO so với A2C là giữ được cấu trúc cũ (Actor, Critic và Advantage), đồng thời bảo đảm việc cập nhật chiến lược sẽ không quá khác biệt, điều này mang lại 2 tác dụng:\n",
    "- Cập nhật chiến lược nhỏ giúp việc huấn luyện dễ hội tụ về một giải pháp tối ưu\n",
    "- Cập nhật chiến lược quá lớn có thể khiến chiến lược mới rất xấu, mất nhiều thời gian để khôi phục hoặc thậm chí không thể khôi phục được\n",
    "\n",
    "**Lưu ý**: Trước khi đi sâu hơn vào [bài báo sau đây](https://arxiv.org/pdf/1707.06347), để không nhầm lẫn giữa cách kí hiệu trong báo cáo này và trong bài báo, quy định rằng $J(\\theta)$ là hàm mục tiêu cần cực đại hoá và tương tự $L(\\theta)$ là hàm loss cần cực tiểu hoá (Giống kí hiệu ở phần A2C)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f66ec1",
   "metadata": {},
   "source": [
    "<a id=\"policy-objective-function\"></a>\n",
    "### 2.2.1. Policy Objective Function\n",
    "\n",
    "Công thức: $$J^{PG}(\\theta) = \\hat{E}_{t}[\\log \\pi_{\\theta}(a_{t}|s_{t}).\\hat{A}_{t}]$$\n",
    "Trong đó:\n",
    "- $\\log \\pi_\\theta(a_{t}|s_{t})$ là log xác suất lựa chọn hành động $a_t$ tại trạng thái $s_t$.\n",
    "- $\\hat{A}_{t}$ là ước lượng giá trị Advantage của $a_t$ tại thời điểm $t$ bằng Monte-Carlo (tính toán cho cả một quỹ đạo). Cụ thể hơn thì $\\hat{A}_{t} = G_{t} - V(s_{t})$ trong đó:\n",
    "\t- $G_{t}$ là tổng điểm thưởng có chiết khấu từ thời điểm $t$: $$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\dots = \\sum_{k = 0}^{\\infty}\\gamma^kR_{t+k+1}$$\n",
    "\t- $V(s)$ là ước lượng điểm thưởng có chiết khấu bắt đầu từ trạng thái $s$\n",
    "- Nếu $\\hat{A}_{t}$ dương, điều đó có nghĩa là đạo hàm dương, tức là ta tăng xác suất thực hiện hành động đó, và tương tự với chiều ngược lại.\n",
    "\n",
    "Một vấn đề xảy ra, mặc dù ta muốn mô hình thực hiện hành động có điểm thưởng cao hơn và tránh những hành động có hại, nhưng trong quá trình huấn luyện:\n",
    "- Nếu cập nhật chiến lược nhỏ, quá trình học sẽ rất chậm.\n",
    "- Nếu cập nhật chiến lược lớn, sẽ có rất nhiều phương sai, biến thiên trong quá trình học\n",
    "\n",
    "Chính vì những lí do kể trên, ta sẽ đi tới công thức tiếp theo, giúp đảm bảo sự thay đổi trong chiến lược sẽ diễn ra từ từ và được kiểm soát trong một khoảng nhất định."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefa719",
   "metadata": {},
   "source": [
    "### 2.2.2. Clipped Surrogate Objective Function\n",
    "\n",
    "Trước tiên, ta có khái niệm về hàm tỉ lệ: $$r_{t}(\\theta) = \\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}$$ \n",
    "Tỉ lệ này được tính bằng cách lấy xác suất lựa chọn hành động $a_{t}$ tại trạng thái $s_t$ trong chiến lược hiện tại, chia cho xác suất tương tự nhưng trong chiến lược trước đó. Ta có thể thấy có 2 khả năng xảy ra:\n",
    "- Nếu $r_{t}(\\theta) > 1$, có nhiều khả năng xảy ra hành động $a_{t}$ hơn trong chiến lược hiện tại so với chiến lược trước đó\n",
    "- Nếu $r_t(\\theta) < 1$, có ít khả năng xảy ra hành động $a_{t}$ hơn trong chiến lược hiện tại so với chiến lược trước đó\n",
    "Hàm tỉ lệ này là một cách đơn giản để ước tính sự chệch hướng giữa chiến lược cũ và mới.\n",
    "\n",
    "Tiếp đến, ta có công thức cho Clipped Surrogate Objective Function: $$J^{CLIP}(\\theta) = \\mathbb{\\hat{E}}_{t}[\\min(r_{t}(\\theta)\\hat{A}_{t},clip(r_{t}(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_{t})]$$\n",
    "Trong công thức này, hàm tỉ lệ ở trên được dùng thay thế log xác suất trong hàm **Policy Objective Function** ở trên. Ta xem xét công thức này qua 2 phần trong hàm $\\min$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5945b38",
   "metadata": {},
   "source": [
    "### 2.2.3. The unclipped part (Phần bên trái - không bị cắt)\n",
    "\n",
    "Công thức: $$J^{CPI}(\\theta) = \\mathbb{\\hat{E}}_{t}\\left[\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{old}}(a_{t}|s_{t})}\\hat{A}_{t}\\right] = \\mathbb{\\hat{E}}_{t}\\left[r_{t}(\\theta)\\hat{A}_{t}\\right]$$\n",
    "Tuy nhiên, nếu không có ràng buộc, việc cực đại hoá hàm này sẽ dẫn tới cập nhật chiến lược quá lớn (điều mà ta không muốn), vậy nên ta phải điều chỉnh hàm mục tiêu này để trừng phạt những thay đổi mà khiến cho $r_{t}(\\theta)$ cách 1 quá xa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15838d",
   "metadata": {},
   "source": [
    "### 2.2.4. The clipped part (Phần bên phải - bị cắt)\n",
    "\n",
    "Công thức: $$clip(r_{t}(\\theta), 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_{t}$$\n",
    "Ở công thức này, ta đảm bảo sự thay đổi chiến lược sẽ không đi quá xa, chỉ được nằm trong khoảng $[1−ϵ,1+ϵ]$ . Theo các thí nghiệm trong [bài báo](https://arxiv.org/pdf/1707.06347), với $\\epsilon = 0.2$ ta có điểm chuẩn hoá cao nhất. \n",
    "\n",
    "![](images/image42.png)\n",
    "\n",
    "**Kết luận**: ta lấy $\\min$ của phần không bị cắt và phần bị cắt, vì vậy hàm mục tiêu cuối cùng sẽ là cận dưới của phần không bị cắt. Bằng cách này, nếu hàm tỉ lệ giúp hàm mục tiêu cải thiện tốt hơn, ta sẽ kệ nó, nhưng nếu nó làm hàm mục tiêu tệ hơn, ta sẽ áp dụng $\\min$ để đảm bảo nó không đi quá xa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc668d",
   "metadata": {},
   "source": [
    "### 2.2.5. Trực quan hoá công thức Clipped Surrogate Objective Function\n",
    "\n",
    "![](images/image43.png)\n",
    "\n",
    "Ta quan sát được tổng cộng 6 trường hợp tương ứng (Trong hình, $p_{t}(\\theta)$ là $r_{t}(\\theta)$). Xét tại thời điểm $t$:\n",
    "\n",
    "- Trường hợp 1 và 2: Hàm tỉ lệ đã nằm sẵn trong khoảng $[1−\\epsilon,1 + \\epsilon]$\n",
    "\t- Trường hợp 1: Hàm advantage dương $\\to$ Hành động hiện tại tốt hơn trung bình các hành động khác $\\to$ khuyến khích chiến lược hiện tại tăng xác suất chọn hành động hiện tại .\n",
    "\t- Trường hợp 2: Hàm advantage âm $\\to$ Hành động hiện tại tệ hơn trung bình các hành động khác $\\to$ KHÔNG khuyến khích chiến lược hiện tại tăng xác suất chọn hành động hiện tại.\n",
    "- Trường hợp 3 và 4: Hàm tỉ lệ nằm dưới khoảng ($r_{t}(\\theta) < 1 - \\epsilon$) $\\to$ Xác suất chọn hành động hiện tại với chiến lược hiện tại ít hơn nhiều so với chiến lược cũ.\n",
    "\t- Trường hợp 3: Hàm advantage dương $\\to$ Tăng xác suất chọn hành động hiện tại.\n",
    "\t- Trường hợp 4: Hàm advantage âm $\\to$ Xác suất đã thấp sẵn, ta càng không muốn giảm xác suất đó đi, vì thế ta không cập nhật hệ số.\n",
    "- Trường hợp 5 và 6: Hàm tỉ lệ nằm trên khoảng ($r_{t}(\\theta) > 1 + \\epsilon$) $\\to$ Xác suất chọn hành động hiện tại với chiến lược hiện tại cao hơn nhiều so với chiến lược cũ.\n",
    "\t- Trường hợp 5: Hàm advantage dương $\\to$ Xác suất đã cao sẵn, ta càng không muốn tăng xác suất đó lên, vì thế ta không cập nhật hệ số.\n",
    "\t- Trường hợp 6: Hàm advantage âm $\\to$ Giảm xác suất chọn hành động hiện tại.\n",
    "\n",
    "**Kết luận**: \n",
    "- Ta chỉ cập nhật chiến lược đối với phần không bị cắt. Cụ thể hơn:\n",
    "\t- Ta cập nhật chiến lược nếu hàm tỉ lệ nằm trong khoảng $[1−\\epsilon,1 + \\epsilon]$.\n",
    "\t- Ta cập nhật chiến lược nếu hàm tỉ lệ nằm ngoài khoảng $[1−\\epsilon,1 + \\epsilon]$, nhưng hàm advantage giúp ta tiến gần hơn khoảng này.\n",
    "- Ta giới hạn khoảng cách mà chiến lược mới có thể thay đổi so với chiến lược cũ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61eeaa",
   "metadata": {},
   "source": [
    "### 2.2.6. Final PPO's Actor Critic Objective Function\n",
    "\n",
    "Công thức cuối cùng ta có: $$J^{CLIP + VF + S}_{t}(\\theta) = \\mathbb{\\hat{E}}_{t}\\left[J^{CLIP}_{t}(\\theta) - c_{1}L^{VF}_{t}(\\theta) + c_{2}S[\\pi_{\\theta}](s_{t})\\right] $$\n",
    "Trong đó:\n",
    "- $\\theta$ là tham số mô hình.\n",
    "- $c_1, c_2$ là hệ số.\n",
    "- $J^{CLIP}_{t}(\\theta)$ là **hàm mục tiêu Clipped Surrogate Objective Function** (phần 2.2.2.)\n",
    "- $L^{VF}_{t}(\\theta)$ là hàm mất mát với sai số bình phương ($L^{VF}_{t}(\\theta) = (V_{\\theta}(s_{t}) -V^{target})^2$).\n",
    "- $S[\\pi_{\\theta}](s_{t})$ là entropy để đảm bảo mô hình có thể đi khám phá.\n",
    "\n",
    "Nhưng đây là hàm mục tiêu, vậy hàm mất mát là gì? Và ta sẽ tối ưu nó ra sao?\n",
    "\n",
    "Rất đơn giản, chỉ cần để dấu trừ đằng trước (tức là lật tất cả các dấu bên trong), ta sẽ có hàm mất mát để có thể cực tiểu hoá. $$L^{CLIP + VF + S}_{t}(\\theta) = \\mathbb{\\hat{E}}_{t}\\left[-J^{CLIP}_{t}(\\theta) + c_{1}L^{VF}_{t}(\\theta) - c_{2}S[\\pi_{\\theta}](s_{t})\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70458578",
   "metadata": {},
   "source": [
    "## 2.3. Generalised Advantage Estimation (GAE)\n",
    "\n",
    "Thay vì công thức tính Advantage $\\hat{A}_{t}$ thông thường, ta dùng một công thức để ước lượng giá trị của nó trong [bài báo dưới đây](https://arxiv.org/pdf/1506.02438). \n",
    "\n",
    "Với công thức tính Temporal Difference Error (TD Error) ở trên và ta biết rằng TD Error có thể dùng để ước lượng $\\hat{A}_{t}$, ta có công thức sau:\n",
    "\n",
    "- Công thức ước lượng $\\hat{A}_{t}$ cho $k$ bước: \n",
    "\n",
    "![](images/image44.png)\n",
    "- Công thức khi $k \\to \\infty$: \n",
    "\n",
    "![](images/image45.png)\n",
    "- Công thức GAE: Công thức này được định nghĩa là trung bình ước lượng của $\\hat{A}_{t}$ trong $k$ bước có trọng số luỹ thừa: \n",
    "\n",
    "![](images/image46.png)\n",
    "- Khi ta thay đổi siêu tham số $\\lambda = 0$ và $\\lambda = 1$, ta có: \n",
    "\t- Với $\\lambda = 0$, phương trình chính là TD Error dùng để ước lượng $\\hat{A}_{t}$ cho 1 bước.\n",
    "\t- Với $\\lambda = 1$, phương trình chính là TD Error Monte-Carlo (tính toán cho cả 1 quỹ đạo).\n",
    "\n",
    "![](images/image47.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f9a70",
   "metadata": {},
   "source": [
    "# 3. Áp dụng thuật toán vào code\n",
    "\n",
    "Code được trình bày theo thuật toán trong [bài báo này](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf).\n",
    "\n",
    "![](images/image48.png)\n",
    "\n",
    "Một số thay đổi:\n",
    "- Để đơn giản hoá, chỉ dùng 1 agent để thu thập training data ($N = 1$).\n",
    "- Thay vì tính $V^{target}_{t}$ trước sau đó mới tính $A_{t}$ thì trong code, nhờ áp dụng thuật toán GAE\n",
    "nên ta sẽ tính $\\hat{A_{t}}$ trước, sau đó mới tính $V^{target}_{t}$ bằng cách cộng thêm với $V_{w}(s_{t})$ (thông qua Critic Network để lấy value của $s_t$).\n",
    "- Thay vì xét với từng $example$ $e \\in M$, ta sẽ tính toán và tối ưu hoá trực tiếp với minibatch $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fed574",
   "metadata": {},
   "source": [
    "## 3.1. Code A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5168d602",
   "metadata": {},
   "source": [
    "# Mô tả quá trình\n",
    "\n",
    "Thuật toán A2C em áp dụng để huấn luyện mô hình cho kết quả rất không tốt, mô hình hầu như không học được và cho rewards âm. Vì phần code của thuật toán này và thuật toán PPO khá giống nhau nên em xin trình bày chi tiết và đầy đủ hơn ở phần sau. Phiên bản này cũng là một phiên bản chưa chỉn chu, hoàn thiện, chưa có docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and dependencies\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as distributions\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "# Training environment (wind disabled)\n",
    "env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,     \n",
    "    gravity=-10.0,        \n",
    "    enable_wind=False,   \n",
    "    wind_power=15.0,      \n",
    "    turbulence_power=1.0, \n",
    "    render_mode=\"rgb_array\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation environment (wind enabled)\n",
    "video_env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,     \n",
    "    gravity=-10.0,        \n",
    "    enable_wind=False,   \n",
    "    wind_power=15.0,      \n",
    "    turbulence_power=1.0, \n",
    "    render_mode=\"rgb_array\" \n",
    ")\n",
    "\n",
    "# Record video every 100 epochs\n",
    "video_env = RecordVideo(video_env, video_folder=\"a2c_1e6_64_2048_3e4_gae\", episode_trigger=lambda x: x % 1000 == 0)   \n",
    "space_dim = env.observation_space.shape[0]      # Observation space: 8-dimensional vector\n",
    "action_dim = env.action_space.n                 # Action space: 4 discrete actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68709c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters (following paper)\n",
    "gamma = 0.99                # Discount factor\n",
    "lr = 3e-4                   # Learning rate\n",
    "lamb = 0.95                 # Generalised Advantage Estimation (GAE) lambda\n",
    "epsilon = 0.2               # Clipping value\n",
    "h = 0.01                    # Entropy coefficient\n",
    "max_timesteps = 1e6         # Maximal number of iterations\n",
    "eval_episodes = 100         # Episodes for evaluation\n",
    "N = 1                       # Number of agents collecting training data\n",
    "T = 2048                    # Maximal trajectory length\n",
    "K = 10                      # Number of epoches per update\n",
    "minibatch_size = 64         # Size of a mini batch\n",
    "number_minibatches = N * T / minibatch_size     # Number of mini batches\n",
    "actor_losses = []           # For plotting\n",
    "critic_losses = []\n",
    "eval_rewards = []            \n",
    "index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b264ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network to select an action\n",
    "ActorNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "\n",
    "# The network to get value of a state\n",
    "CriticNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "# Optimizer using Adam Gradient Descent\n",
    "actor_optimizer = optim.Adam(ActorNetwork.parameters(), lr=lr)\n",
    "critic_optimizer = optim.Adam(CriticNetwork.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state_tensor):\n",
    "    value = CriticNetwork(state_tensor)\n",
    "    action_pred = ActorNetwork(state_tensor)                    \n",
    "    dist = distributions.Categorical(logits=action_pred)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    return action, value, log_prob\n",
    "def collect_training_data(state, action, reward, log_prob, value, done, states, actions, rewards, log_probs, values, dones):\n",
    "    states.append(state)         # Collect states\n",
    "    actions.append(action)        # Collect actions from Actor Network\n",
    "    rewards.append(reward)        # Collect rewards\n",
    "    log_probs.append(log_prob)      # Collect lob_probs\n",
    "    values.append(value)         # Collect values from Critic Network\n",
    "    dones.append(done)          # Collect done (0 or 1)\n",
    "def compute_GAE(next_value, rewards, values, dones):\n",
    "    advantages = []\n",
    "    GAE = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]      # TD error\n",
    "        GAE = delta + gamma * lamb * (1 - dones[t]) * GAE\n",
    "        advantages.insert(0, GAE)\n",
    "        next_value = values[t].item()\n",
    "    return advantages\n",
    "def compute_advantages(next_state_tensor: torch.Tensor, rewards: list, values: list, dones: list):\n",
    "    next_value = CriticNetwork(next_state_tensor)                                           # Calculate the next value\n",
    "    advantages = compute_GAE(next_value, rewards=rewards, values=values, dones=dones)       # Calculate advantage using GAE\n",
    "    advantages = torch.FloatTensor(advantages)              \n",
    "    return advantages\n",
    "def get_parameterized_policy(batch_states, batch_actions):\n",
    "    optimized_value = CriticNetwork(batch_states)\n",
    "    optimized_action_preds = ActorNetwork(batch_states)\n",
    "    optimized_dist = distributions.Categorical(logits=optimized_action_preds)\n",
    "    optimized_log_probs = optimized_dist.log_prob(batch_actions)\n",
    "\n",
    "    return optimized_value, optimized_dist, optimized_log_probs\n",
    "def evaluation(eval_episodes):\n",
    "    total_reward = 0\n",
    "    for _ in range(eval_episodes):\n",
    "        episode_reward = 0\n",
    "        state, _ = video_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_pred = ActorNetwork(state_tensor)\n",
    "            action = torch.argmax(action_pred).item()\n",
    "            state, reward, terminated, truncated, _ = video_env.step(action)\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "\n",
    "        total_reward += episode_reward\n",
    "    return total_reward / eval_episodes  \n",
    "\n",
    "def moving_average(data, window_size=10):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()      # Initialize state s_t\n",
    "timesteps = 0\n",
    "\n",
    "while timesteps < max_timesteps:\n",
    "    states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n",
    "    print(timesteps)\n",
    "\n",
    "    for _ in range(T):              # Collect T timesteps for 1 rollout\n",
    "        action, value, log_prob = select_action(torch.FloatTensor(state)) # Get action a_t, value V(s)_t, and old policy given action a_t\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())      # Advance simulation one time step\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Collect training data\n",
    "        collect_training_data(state, action, reward, log_prob, value, done, states, actions, rewards, log_probs, values, dones)  \n",
    "\n",
    "        state = next_state      # Move to next state\n",
    "        timesteps += 1          # Increase timesteps\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()      # If an episode is done, reset the state\n",
    "    \n",
    "    # Compute advantages\n",
    "    advantages = compute_advantages(next_state_tensor=torch.FloatTensor(state), rewards=rewards, values=values, dones=dones)     # Calculate advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)               # Normalize advantages\n",
    "    returns = advantages + torch.FloatTensor(values)        # Calculate V_target_t = A_t + V_w(s_t)\n",
    "\n",
    "    # Convert data for training \n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    old_log_probs = torch.FloatTensor(log_probs)\n",
    "    returns = returns.detach()\n",
    "\n",
    "    # Optimizing the surrogate loss\n",
    "    for k in range(K):          \n",
    "        for i in torch.randperm(len(states)).split(minibatch_size):        # Sample over batches with size 'minibatch_size'      \n",
    "            batch_states = states[i]\n",
    "            batch_actions = actions[i]\n",
    "            batch_old_log_probs = old_log_probs[i]\n",
    "            batch_returns = returns[i]\n",
    "            batch_advantages = advantages[i]\n",
    "\n",
    "            optimized_value, optimized_dist, optimized_log_probs = get_parameterized_policy(batch_states=batch_states, batch_actions=batch_actions)\n",
    "            H_entropy = optimized_dist.entropy().mean()\n",
    "\n",
    "            actor_loss = -(optimized_log_probs * batch_advantages).mean() - h * H_entropy\n",
    "            critic_loss = (optimized_value.squeeze() - batch_returns).pow(2).mean()  \n",
    "\n",
    "            actor_losses.append(actor_loss.item())\n",
    "            critic_losses.append(critic_loss.item())\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "    if timesteps % T == 0:\n",
    "        try:\n",
    "            avg_rewards = evaluation(eval_episodes)\n",
    "            print(f\"In current timestep: {timesteps}, average reward: {avg_rewards}\", flush=True)\n",
    "            index.append(timesteps)\n",
    "            eval_rewards.append(avg_rewards)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluation: {e}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8fd3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_average(actor_losses))\n",
    "plt.title(\"Actor Loss\")\n",
    "plt.xlabel(\"Rollout\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a160a6c",
   "metadata": {},
   "source": [
    "![](images/image49.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ba1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_average(critic_losses))\n",
    "plt.title(\"Critic Loss\")\n",
    "plt.xlabel(\"Rollout\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6858a",
   "metadata": {},
   "source": [
    "![](images/image50.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d987ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(index, eval_rewards)\n",
    "plt.title(\"Evaluation Rewards\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02bc44",
   "metadata": {},
   "source": [
    "![](images/image51.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7f28",
   "metadata": {},
   "source": [
    "## 3.2 Code PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aaa3b6",
   "metadata": {},
   "source": [
    "### Mô tả quá trình\n",
    "\n",
    "Đây là phiên bản code PPO đã được tối ưu và hoàn thiện nhất có thể. Trong phần code này, em thực hiện training theo thuật toán ở phía trên. Trong quá trình huấn luyện, em lần lượt thay đổi với từng yếu tố khác nhau: \n",
    "- T = 2048 hoặc 4096 (số lượng timesteps trong một rollout).\n",
    "- minibatch_size = 64 hoặc 256 (kích cỡ của một minibatch).\n",
    "- Có gió hoặc không có gió.\n",
    "\n",
    "Nhờ thay đổi những siêu tham số này, em tổng hợp kết quả của 8 lần training và thời gian training của chúng. Kết quả training được đánh giá qua 5 tiêu chí khác nhau:\n",
    "- Loss của Actor Network\n",
    "- Loss của Critic Network\n",
    "- Điểm trung bình qua thời gian\n",
    "- Độ lệch chuẩn qua thời gian\n",
    "- Tỉ lệ hạ cánh thành công qua thời gian\n",
    "\n",
    "### Quá trình training sẽ diễn ra tuần tự như sau:\n",
    "\n",
    "1. Reset lại môi trường, khởi tạo state đầu tiên.\n",
    "2. Cho timestep chạy từ 0 đến max_timesteps.\n",
    "3. Với mỗi một lần rollout tương ứng với ***T*** timesteps:\n",
    "    1. Dùng hàm **select_action** để chọn một ***action*** ngẫu nhiên theo chiến lược của Actor Network, trả về ***value*** tại state hiện tại theo Critic Network, và ***log-probability*** của action đó.\n",
    "    2. Từ action này, lấy ra ***next_state*** tiếp theo của môi trường, điểm thưởng cho action đó, cũng như trạng thái của môi trường (đã ***terminated*** hoặc ***truncated*** chưa).\n",
    "    3. Dữ liệu huấn luyện (***state, action, reward, log_prob, value, done***) sẽ được lưu vào danh sách để phục vụ huấn luyện sau này.\n",
    "    4. Nếu môi trường kết thúc (***done***), reset lại môi trường và lấy ***state*** mới.\n",
    "    5. Tăng số lượng ***timesteps*** để theo dõi quá trình huấn luyện đã diễn ra bao lâu.\n",
    "4. Sau mỗi lượt rollout, ta sẽ tính Advantage và Returns\n",
    "    1. Tính ***advantages*** bằng Generalised Advantage Estimate (GAE) từ ***rewards*** và ***values***.\n",
    "    2. Chuẩn hoá ***advantages*** để giúp quá trình training được ổn định.\n",
    "    3. Tính ***returns*** = ***advantages*** + ***values*** (Đây cũng chính là $V_{target}$ cho Critic).\n",
    "5. Chuyển đổi dữ liệu huấn luyện thu thập được qua Tensor để dễ dàng làm việc với mạng nơ-ron.\n",
    "6. Tối ưu hoá chính sách: \n",
    "    1. Lặp với ***K*** epochs.\n",
    "    2. Chia dữ liệu thành các minibatch ngẫu nhiên với kích cỡ là ***minibatch_size***.\n",
    "    3. Với mỗi minibatch:\n",
    "        1. Từ mô hình mạng nơ-ron hiện tại, tính ***optimized_value***, ***optimized_dist***, ***optimized_log_probs***.\n",
    "        2. Tính ***L_clip*** là PPO Loss theo clipping surrogate function, ***L_v*** là Critic loss và ***H_entropy*** là entropy của chiến lược để khuyến khích khám phá (exploration thay vì exploitation).\n",
    "        3. Tính ***actor_loss = L_clip - h * H_entropy*** trong đó ***h*** là hệ số entropy.\n",
    "        4. Tính ***critic_loss = v * L_v*** trong đó ***v*** là hệ số của Critic loss.\n",
    "        5. Cập nhật ***Actor Network*** và ***Critic Network*** bằng backpropagation.\n",
    "        6. Lưu lại giá trị loss để plot.\n",
    "7. Sau mỗi lượt rollout, đánh giá mô hình tại thời điểm đó.\n",
    "    1. Gọi hàm ***evaluation_and_save()*** để đánh giá các kết quả sau qua 100 lượt chơi thử:\n",
    "        1. Điểm thưởng trung bình (***avg_rewards***)\n",
    "        2. Độ lệch chuẩn (***std_rewards***)\n",
    "        3. Tỉ lệ hạ cánh thành công (***success_rate***)\n",
    "    2. In kết quả đánh giá để kiểm tra trong quá trình training.\n",
    "    3. Lưu những kết quả này để plot.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb21b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and dependencies\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as distributions\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed29179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters (following paper)\n",
    "gamma = 0.99                # Discount factor\n",
    "lr = 3e-4                   # Learning rate\n",
    "lamb = 0.95                 # Generalised Advantage Estimation (GAE) lambda\n",
    "epsilon = 0.2               # Clipping value\n",
    "h = 0.01                    # Entropy coefficient\n",
    "v = 0.5                     # Value loss coefficient\n",
    "max_timesteps = 1e6         # Maximal number of iterations\n",
    "eval_episodes = 100         # Episodes for evaluation\n",
    "N = 1                       # Number of agents collecting training data\n",
    "T = 4096                    # Maximal trajectory length\n",
    "K = 10                      # Number of epoches per update\n",
    "minibatch_size = 64         # Size of a mini batch\n",
    "number_minibatches = N * T / minibatch_size     # Number of mini batches\n",
    "max_reward = 0\n",
    "actor_losses = []          \n",
    "critic_losses = []\n",
    "eval_rewards = []\n",
    "eval_standard_deviations = []\n",
    "success_rates = []            \n",
    "index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ed505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluating environment (wind is enabled/disabled)\n",
    "env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,     \n",
    "    gravity=-10.0,        \n",
    "    enable_wind=False,   \n",
    "    wind_power=15.0,      \n",
    "    turbulence_power=1.0, \n",
    "    render_mode=\"rgb_array\" \n",
    ")\n",
    "\n",
    "video_folder = f\"ppo_wind_{max_timesteps}_{minibatch_size}_{T}_{lr}\" if env.unwrapped.enable_wind else f\"ppo_{max_timesteps}_{minibatch_size}_{T}_{lr}\"\n",
    "\n",
    "# Record video every 1000 epochs\n",
    "video_env = RecordVideo(env, video_folder=f\"{video_folder}\", episode_trigger=lambda x: x % 1000 == 0) \n",
    "\n",
    "# Video for the best model\n",
    "best_env = RecordVideo(env, video_folder=f\"best_of_{video_folder}\")\n",
    "\n",
    "space_dim = env.observation_space.shape[0]                  # Observation space: 8-dimensional vector\n",
    "action_dim = env.action_space.n                             # Action space: 4 discrete actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network to select an action\n",
    "ActorNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "\n",
    "# The network to get value of a state\n",
    "CriticNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "# Optimizer using Adam Gradient Descent\n",
    "actor_optimizer = optim.Adam(ActorNetwork.parameters(), lr=lr)\n",
    "critic_optimizer = optim.Adam(CriticNetwork.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace99e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state_tensor: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Select a random action following a policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_tensor : torch.FloatTensor\n",
    "        An 8-dimensional vector of observation space.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    action : torch.Tensor\n",
    "        The discrete action taken 0, 1, 2, 3.\n",
    "    value : torch.Tensor \n",
    "        Value estimated at the state from the CriticNetwork.\n",
    "    log_prob : torch.Tensor: \n",
    "        Log-probability of the chosen action.\n",
    "    \"\"\"\n",
    "    value = CriticNetwork(state_tensor)\n",
    "    action_pred = ActorNetwork(state_tensor)    # Tensor of raw logits              \n",
    "    dist = distributions.Categorical(logits=action_pred)    # Form a categorical distribution from the logits (perform softmax internally)\n",
    "    action = dist.sample()              # Sample an action from the distribution\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    return action, value, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb262157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_training_data(state, action, reward, log_prob, value, done, states, actions, rewards, log_probs, values, dones):\n",
    "    \"\"\"\n",
    "    Collect training data during a rollout.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : numpy.ndarray\n",
    "        The environment state at a timestep.\n",
    "    action : torch.Tensor\n",
    "        The action taken at the given state.\n",
    "    reward : numpy.float64\n",
    "        Reward received for taking the action.\n",
    "    log_prob : torch.Tensor\n",
    "        Log-probability of the action.\n",
    "    value : torch.Tensor\n",
    "        Value estimated for the state.\n",
    "    done : bool\n",
    "        Whether the episode has ended at this timestep.\n",
    "    states : list of numpy.ndarray\n",
    "        List to store collected states.\n",
    "    actions : list of torch.Tensor\n",
    "        List to store collected actions.\n",
    "    rewards : list of numpy.float64\n",
    "        List to store collected rewards.\n",
    "    log_probs : list of torch.Tensor\n",
    "        List to store collected log-probabilities.\n",
    "    values : list of torch.Tensor\n",
    "        List to store collected state-value estimates.\n",
    "    dones : list of bool\n",
    "        List to store episode termination flags.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None  \n",
    "    \"\"\"\n",
    "    states.append(state)         # Collect states\n",
    "    actions.append(action)        # Collect actions from Actor Network\n",
    "    rewards.append(reward)        # Collect rewards\n",
    "    log_probs.append(log_prob)      # Collect lob_probs\n",
    "    values.append(value)         # Collect values from Critic Network\n",
    "    dones.append(done)          # Collect done (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_GAE(next_value, rewards, values, dones) -> list:\n",
    "    \"\"\"\n",
    "    Compute the Generalized Advantage Estimate (GAE).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    next_value : numpy.ndarray\n",
    "        Value estimate of the next state after the last timestep.\n",
    "    rewards : list of numpy.float64\n",
    "        Collected rewards at each timestep.\n",
    "    values : list of torch.Tensor\n",
    "        Collected state-value estimates at each timestep.\n",
    "    dones : list of bool\n",
    "        Episode termination flags (True if episode ended at that timestep).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    advantages : list of numpy.float64\n",
    "        Advantage estimates, one per timestep.   \n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    GAE = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]\n",
    "        GAE = delta + gamma * lamb * (1 - dones[t]) * GAE\n",
    "        advantages.insert(0, GAE)\n",
    "        next_value = values[t].item()\n",
    "    return advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6cdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(next_state_tensor: torch.Tensor, rewards: list, values: list, dones: list) -> list[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get Generalised Advantage Estimate and convert to Tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    next_state_tensor : torch.Tensor\n",
    "        An 8-dimensional vector of observation space indicating the next state.\n",
    "    rewards : list of numpy.float64\n",
    "        List to store collected rewards.\n",
    "    values : list of torch.Tensor\n",
    "        List to store collected value estimates.\n",
    "    dones : list of bool\n",
    "        List to store episode termination indicators.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    advantages : list of numpy.float64\n",
    "        List of advantage estimates, one for each timestep.   \n",
    "    \"\"\"\n",
    "    next_value = CriticNetwork(next_state_tensor)                                           # Calculate the next value\n",
    "    advantages = compute_GAE(next_value, rewards=rewards, values=values, dones=dones)       # Calculate advantage using GAE\n",
    "    advantages = torch.FloatTensor(advantages)              \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameterized_policy(batch_states, batch_actions) -> tuple[torch.Tensor, torch.distributions.Categorical, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Under the current policy, calculate value estimate of a batch of states, \n",
    "    distribution of a batch of actions, and their Log-probability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_states : torch.FloatTensor\n",
    "        Batch of states, where each state is an 8-dimensional vector.\n",
    "    batch_actions : torch.FloatTensor\n",
    "        Batch of actions taken corresponding to the batch states.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    optimized_value : torch.Tensor\n",
    "        Predicted state values from the Critic Network.\n",
    "    optimized_dist : torch.distributions.Categorical\n",
    "        Action distribution over discrete actions from the Actor Network.\n",
    "    optimized_log_probs : torch.Tensor\n",
    "        Log-probabilities of the batch actions under the current policy. \n",
    "    \"\"\"\n",
    "    optimized_value = CriticNetwork(batch_states)\n",
    "    optimized_action_preds = ActorNetwork(batch_states)\n",
    "    optimized_dist = distributions.Categorical(logits=optimized_action_preds)\n",
    "    optimized_log_probs = optimized_dist.log_prob(batch_actions)\n",
    "\n",
    "    return optimized_value, optimized_dist, optimized_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b10674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clipped_surrogate_loss(batch_old_log_probs, batch_returns, batch_advantages, optimized_value, optimized_dist, optimized_log_probs):\n",
    "    \"\"\"\n",
    "    Computes the PPO clipped surrogate objective, value function loss, and entropy bonus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_old_log_probs : torch.Tensor\n",
    "        Batch of Log-probabilities of actions under the old policy.\n",
    "    batch_returns : torch.Tensor\n",
    "        Batch of all estimated returns (reward to go) for each state.\n",
    "    batch_advantages : torch.Tensor\n",
    "        Batch of GAE for each action.\n",
    "    optimized_value : torch.Tensor\n",
    "        Value estimate from the current Critic Network.\n",
    "    optimized_dist : torch.distributions.Categorical\n",
    "        Action distribution from the current Actor Network.\n",
    "    optimized_log_probs : torch.Tensor\n",
    "        Batch of Log-probabilities of actions under the current policy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    L_clip : torch.Tensor\n",
    "        Clipped surrogate loss for the policy.\n",
    "    L_v : torch.Tensor\n",
    "        Value function loss using Mean Squared Error.\n",
    "    H_entropy : torch.Tensor\n",
    "        Mean entropy bonus to encourage exploration.  \n",
    "    \"\"\"\n",
    "    ratio = (optimized_log_probs - batch_old_log_probs).exp()                                   # Ratio = divergence between old and current policy\n",
    "    unclipped_objective = ratio * batch_advantages                                              # The unclipped part\n",
    "    clipped_objective = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * batch_advantages         # The clipped part: Clip the ratio to range [1 - ε, 1 + ε]\n",
    "\n",
    "    L_clip = -torch.min(unclipped_objective, clipped_objective).mean()          # Take the smaller part\n",
    "    L_v = (optimized_value.squeeze() - batch_returns).pow(2).mean()             # Squared-error value loss\n",
    "    H_entropy = optimized_dist.entropy().mean()                                 # Entropy bonus: Ensure sufficient exploration\n",
    "\n",
    "    return L_clip, L_v, H_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_and_save(eval_episodes, max_reward):\n",
    "    \"\"\"\n",
    "    Utility function to evaluate average rewards over 100 episodes, and save the model with the best reward recorded.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eval_episodes : int\n",
    "        Number of episodes to evaluate.\n",
    "    max_reward : int\n",
    "        Maximum reward ever recorded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_reward : float\n",
    "        Average reward over the evaluated episodes.\n",
    "    std_reward : float\n",
    "        Standard deviation of the rewards.\n",
    "    success_rate : float\n",
    "        Percentage of episodes with a reward of at least 200.\n",
    "    \"\"\"\n",
    "    success_rate = 0\n",
    "    reward_store = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(eval_episodes):\n",
    "            episode_reward = 0\n",
    "            state, _ = video_env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(state)\n",
    "                action_pred = ActorNetwork(state_tensor)\n",
    "                action = torch.argmax(action_pred).item()\n",
    "                state, reward, terminated, truncated, _ = video_env.step(action)\n",
    "                episode_reward += reward\n",
    "                done = terminated or truncated\n",
    "\n",
    "            reward_store.append(episode_reward)\n",
    "\n",
    "            if episode_reward >= 200:\n",
    "                success_rate += 1\n",
    "\n",
    "            if episode_reward > max_reward:\n",
    "                max_reward = episode_reward\n",
    "                torch.save({'Actor': ActorNetwork.state_dict(), 'Critic': CriticNetwork.state_dict()}, \"best_model_now.pth\")\n",
    "    return sum(reward_store)/eval_episodes, np.std(reward_store), success_rate / eval_episodes * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d85a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_best_results():\n",
    "    \"\"\"\n",
    "    Show the mp4 video of the model with the best result, over the whole training process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(\"best_model_now.pth\")\n",
    "\n",
    "    ActorNetwork.load_state_dict(state_dict=state_dict['Actor'])\n",
    "    CriticNetwork.load_state_dict(state_dict=state_dict['Critic'])\n",
    "\n",
    "    best_reward = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        state, _ = best_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_pred = ActorNetwork(state_tensor)\n",
    "            action = torch.argmax(action_pred).item()\n",
    "            state, reward, terminated, truncated, _ = best_env.step(action)\n",
    "            best_reward += reward\n",
    "            done = terminated or truncated\n",
    "\n",
    "    print(f\"Best reward collected: {best_reward}\")\n",
    "\n",
    "    best_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd4bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, max_timesteps, K, T, minibatch_size, max_reward, actor_losses, critic_losses, index, eval_rewards, eval_standard_deviations, success_rates):\n",
    "    \"\"\"Main function to train the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gymnasium.Env\n",
    "        The Lunar Lander V3 OpenAI Gymnasium environment.\n",
    "    max_timesteps : int\n",
    "        Maximum timesteps to train.\n",
    "    K : int\n",
    "        Number of epochs to optimize model.\n",
    "    T : int\n",
    "        Number of timesteps for a rollout.\n",
    "    minibatch_size : int\n",
    "        Size of a mini batch.\n",
    "    max_reward : int\n",
    "        Maximum reward ever recorded.\n",
    "    actor_losses : list\n",
    "        List to collect loss from Actor Network.\n",
    "    critic_losses : list\n",
    "        List to collect loss from Critic Network.\n",
    "    index : list\n",
    "        List to collect indices for plotting.\n",
    "    eval_rewards : list\n",
    "        List to collect average rewards over the training process.\n",
    "    eval_standard_deviations : list\n",
    "        List to collect standard deviations over the training process.\n",
    "    success_rates : list\n",
    "        List to collect success landing ratio over the training process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    state, _ = env.reset()      # Initialize state s_t\n",
    "    timesteps = 0\n",
    "\n",
    "    while timesteps < max_timesteps:\n",
    "        states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n",
    "        print(timesteps)\n",
    "\n",
    "        for _ in range(T):              # Collect T timesteps for 1 rollout\n",
    "            action, value, log_prob = select_action(torch.FloatTensor(state)) # Get action a_t, value V(s)_t, and old policy given action a_t\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())      # Advance simulation one time step\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Collect training data\n",
    "            collect_training_data(state, action, reward, log_prob, value, done, states, actions, rewards, log_probs, values, dones)  \n",
    "\n",
    "            state = next_state      # Move to next state\n",
    "            timesteps += 1          # Increase timesteps\n",
    "\n",
    "            if done:\n",
    "                state, _ = env.reset()      # If an episode is done, reset the state\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = compute_advantages(next_state_tensor=torch.FloatTensor(state), rewards=rewards, values=values, dones=dones)     # Calculate advantages\n",
    "        advantages = ((advantages - advantages.mean()) / (advantages.std() + 1e-8))               # Normalize advantages\n",
    "        returns = advantages + torch.FloatTensor(values)        # Calculate V-target_t = A_t + V-w(s_t)\n",
    "\n",
    "        # Convert data for training \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        old_log_probs = torch.FloatTensor(log_probs)\n",
    "        returns = returns.detach()\n",
    "\n",
    "        # Optimizing the surrogate loss\n",
    "        for k in range(K):          \n",
    "            for i in torch.randperm(len(states)).split(minibatch_size):        # Sample over batches with size 'minibatch_size'      \n",
    "                batch_states = states[i]\n",
    "                batch_actions = actions[i]\n",
    "                batch_old_log_probs = old_log_probs[i]\n",
    "                batch_returns = returns[i]\n",
    "                batch_advantages = advantages[i]\n",
    "\n",
    "                optimized_value, optimized_dist, optimized_log_probs = get_parameterized_policy(batch_states=batch_states, batch_actions=batch_actions)\n",
    "            \n",
    "                L_clip, L_v, H_entropy = calculate_clipped_surrogate_loss(batch_old_log_probs, batch_returns, batch_advantages, optimized_value, optimized_dist, optimized_log_probs)\n",
    "\n",
    "                # Loss = L_clip + v * L_v - h * H_entropy\n",
    "\n",
    "                # Actor loss\n",
    "                actor_loss = L_clip - h * H_entropy\n",
    "                actor_losses.append(actor_loss.item())\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # Critic loss\n",
    "                critic_loss = L_v * v\n",
    "                critic_losses.append(critic_loss.item())\n",
    "                critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                critic_optimizer.step()\n",
    "\n",
    "        if timesteps % T == 0:\n",
    "            try:\n",
    "                avg_rewards, std_rewards, success_rate = evaluation_and_save(eval_episodes, max_reward=max_reward)\n",
    "                print(f\"In current timestep: {timesteps}, Average reward: {avg_rewards}, Standard Deviation: {std_rewards}, Success rate: {success_rate}\", flush=True)\n",
    "                index.append(timesteps)\n",
    "                eval_rewards.append(avg_rewards)\n",
    "                eval_standard_deviations.append(std_rewards)\n",
    "                success_rates.append(success_rate)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in evaluation: {e}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ec43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size=10):\n",
    "    \"\"\"\n",
    "    Compute the moving average of the input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array_like\n",
    "        Input array or list of numerical data.\n",
    "    window_size : int, optional\n",
    "        Size of the moving average window, by default 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Array of moving averages with length len(data) - window_size + 1.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env, max_timesteps, K, T, minibatch_size, max_reward, actor_losses, critic_losses, index, eval_rewards, eval_standard_deviations, success_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe52416",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_average(actor_losses), c=\"blue\")\n",
    "plt.title(\"Actor Loss\")\n",
    "plt.xlabel(\"Rollout\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_average(critic_losses), c=\"orange\")\n",
    "plt.title(\"Critic Loss\")\n",
    "plt.xlabel(\"Rollout\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932806b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(index, eval_rewards, c=\"red\")\n",
    "plt.ylim(top=300)\n",
    "plt.title(\"Evaluation Rewards\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8146a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(index, eval_standard_deviations, c=\"pink\")\n",
    "plt.title(\"Standard Deviation over Timesteps\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"STD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a255c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(index, success_rates, c=\"green\")\n",
    "plt.ylim(top=100)\n",
    "plt.title(\"Success Rate over Timesteps\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Success Rates (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fc00d",
   "metadata": {},
   "source": [
    "# 4. Kết quả thu được"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4ba8e",
   "metadata": {},
   "source": [
    "## 4.1. PPO không có gió"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366deaa2",
   "metadata": {},
   "source": [
    "### 4.1.1. 1e6 Timesteps, Batchsize = 64, n_steps = 2048 (Training time: 2h50p)\n",
    "\n",
    "| Actor Loss             | Critic Loss            | Rewards                |\n",
    "| ---------------------- | ---------------------- | ---------------------- |\n",
    "| ![](images/image21.png) | ![](images/image22.png) | ![](images/image23.png) |\n",
    "| **Standard Deviation** | **Success Rate**       |                        |\n",
    "| ![](images/image24.png) | ![](images/image25.png) |                        |\n",
    "\n",
    "**Best reward collected**: *307.8842375695475*\n",
    "\n",
    "<video src=\"best_of_ppo_1e6_64_2048_3e4\\rl-video-episode-0.mp4\" controls></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78679a",
   "metadata": {},
   "source": [
    "### 4.1.2. 1e6 Timesteps, Batchsize = 256, n_steps = 2048 (Training time: 2h22p)\n",
    "\n",
    "| Actor Loss             | Critic Loss            | Rewards                |\n",
    "| ---------------------- | ---------------------- | ---------------------- |\n",
    "| ![](images/image26.png) | ![](images/image27.png) | ![](images/image28.png) |\n",
    "| **Standard Deviation** | **Success Rate**       |                        |\n",
    "| ![](images/image29.png) | ![](images/image30.png) |                        |\n",
    "\n",
    "**Best reward collected**: *272.87406842041014*\n",
    "\n",
    "<video src=\"best_of_ppo_1e6_256_2048_3e4\\rl-video-episode-0.mp4\" controls></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5edf5",
   "metadata": {},
   "source": [
    "### 4.1.3. 1e6 Timesteps, Batchsize = 64, n_steps = 4096 (Training time: 1h20p)\n",
    "\n",
    "| Actor Loss             | Critic Loss            | Rewards                |\n",
    "| ---------------------- | ---------------------- | ---------------------- |\n",
    "| ![](images/image31.png) | ![](images/image32.png) | ![](images/image33.png) |\n",
    "| **Standard Deviation** | **Success Rate**       |                        |\n",
    "| ![](images/image34.png) | ![](images/image35.png) |                        |\n",
    "\n",
    "**Best reward collected**: *268.36002417594966*\n",
    "\n",
    "<video src=\"best_of_ppo_1e6_64_4096_3e4\\rl-video-episode-0.mp4\" controls></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722329f",
   "metadata": {},
   "source": [
    "### 4.1.4. 1e6 Timesteps, Batchsize = 256, n_steps = 4096 (Training time: 1h40p)\n",
    "\n",
    "| Actor Loss             | Critic Loss            | Rewards                |\n",
    "| ---------------------- | ---------------------- | ---------------------- |\n",
    "| ![](images/image36.png) | ![](images/image37.png) | ![](images/image38.png) |\n",
    "| **Standard Deviation** | **Success Rate**       |                        |\n",
    "| ![](images/image39.png) | ![](images/image40.png) |                        |\n",
    "\n",
    "**Best reward collected**: *247.34049715264368*\n",
    "\n",
    "<video src=\"best_of_ppo_1e6_256_4096_3e4\\rl-video-episode-0.mp4\" controls></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2a384",
   "metadata": {},
   "source": [
    "## 4.2. PPO có gió"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52ca90",
   "metadata": {},
   "source": [
    "### 4.2.1. 1e6 Timesteps, Batchsize = 64, n_steps = 2048 (Training time: 3h12p)\n",
    "\n",
    "| Actor Loss             | Critic Loss            | Rewards                |\n",
    "| ---------------------- | ---------------------- | ---------------------- |\n",
    "| ![](images/image1.png) | ![](images/image2.png) | ![](images/image3.png) |\n",
    "| **Standard Deviation** | **Success Rate**       |                        |\n",
    "| ![](images/image4.png) | ![](images/image5.png) |                        |\n",
    "\n",
    "**Best reward collected**: *273.43429085932473*\n",
    "\n",
    "<video src=\"best_of_ppo_wind_1e6_64_2048_3e4\\rl-video-episode-0.mp4\" controls></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d572cae",
   "metadata": {},
   "source": [
    "### 4.2.2. 1e6 Timesteps, Batchsize = 256, n_steps = 2048 (Training time: 3h20p)\n",
    "\n",
    "| Actor Loss             | Critic Loss            | Rewards                |\n",
    "| ---------------------- | ---------------------- | ---------------------- |\n",
    "| ![](images/image6.png) | ![](images/image7.png) | ![](images/image8.png) |\n",
    "| **Standard Deviation** | **Success Rate**       |                        |\n",
    "| ![](images/image9.png) | ![](images/image10.png) |                        |\n",
    "\n",
    "**Best reward collected**: *277.108768843627*\n",
    "\n",
    "<video src=\"best_of_ppo_wind_1e6_256_2048_3e4\\rl-video-episode-0.mp4\" controls></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a26004",
   "metadata": {},
   "source": [
    "### 4.2.3. 1e6 Timesteps, Batchsize = 64, n_steps = 4096 (Training time: 2h45p)\n",
    "\n",
    "| Actor Loss             | Critic Loss            | Rewards                |\n",
    "| ---------------------- | ---------------------- | ---------------------- |\n",
    "| ![](images/image11.png) | ![](images/image12.png) | ![](images/image13.png) |\n",
    "| **Standard Deviation** | **Success Rate**       |                        |\n",
    "| ![](images/image14.png) | ![](images/image15.png) |                        |\n",
    "\n",
    "**Best reward collected**: *286.28957209302223*\n",
    "\n",
    "<video src=\"best_of_ppo_wind_1e6_64_4096_3e4\\rl-video-episode-0.mp4\" controls></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6db731",
   "metadata": {},
   "source": [
    "### 4.2.4. 1e6 Timesteps, Batchsize = 256, n_steps = 4096 (Training time: 2h40p)\n",
    "\n",
    "| Actor Loss             | Critic Loss            | Rewards                |\n",
    "| ---------------------- | ---------------------- | ---------------------- |\n",
    "| ![](images/image16.png) | ![](images/image17.png) | ![](images/image18.png) |\n",
    "| **Standard Deviation** | **Success Rate**       |                        |\n",
    "| ![](images/image19.png) | ![](images/image20.png) |                        |\n",
    "\n",
    "**Best reward collected**: *271.1365183294953*\n",
    "\n",
    "<video src=\"best_of_ppo_wind_1e6_256_4096_3e4\\rl-video-episode-0.mp4\" controls></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9259053",
   "metadata": {},
   "source": [
    "# 5. Tổng kết quá trình huấn luyện\n",
    "Qua kết quả thu được, em nhận thấy rằng việc sử dụng thuật toán Proximal Policy Optimization (PPO) mang lại hiệu quả huấn luyện cao hơn rõ rệt so với Advantage Actor Critic (A2C). Nguyên nhân chính đến từ tính ổn định cao của PPO: thuật toán này giới hạn độ thay đổi giữa các chính sách cũ và mới thông qua hàm phạt (clipping), từ đó tránh được tình trạng cập nhật quá lớn làm hỏng chính sách đang học. Nhờ vậy, quá trình huấn luyện diễn ra trơn tru hơn, ít dao động mạnh, và giúp Agent dễ dàng hội tụ đến chiến lược tối ưu.\n",
    "\n",
    "Bên cạnh đó, việc lựa chọn và tinh chỉnh các siêu tham số như learning rate, batch size, discount factor, v.v., cũng đóng vai trò rất quan trọng. Với bộ siêu tham số phù hợp, thuật toán PPO đã nhanh chóng đạt được sự hội tụ: Agent không những học được cách chơi mà còn thể hiện được những hành vi thông minh, chủ động đưa ra chiến lược tối ưu trong môi trường.\n",
    "\n",
    "Ngoài ra, qua quá trình thực nghiệm, em cũng nhận thấy PPO có khả năng khái quát tốt hơn, tức là Agent không chỉ học thuộc một vài tình huống cụ thể mà có thể thích ứng linh hoạt với các trạng thái khác nhau trong môi trường (không có gió, có gió...). Điều này cho thấy tiềm năng lớn của PPO khi áp dụng vào các bài toán phức tạp hơn trong thực tế.\n",
    "\n",
    "Tuy nhiên, vẫn còn một số hạn chế cần lưu ý, chẳng hạn như thời gian huấn luyện khá lâu do PPO yêu cầu lượng sample lớn hơn để đạt được sự ổn định. Trong tương lai, có thể xem xét các cải tiến như sử dụng kỹ thuật Early Stopping, điều chỉnh siêu tham số, tối ưu thời gian chạy, ....\n",
    "\n",
    "Nếu đã đọc đến đây, em xin trân trọng cảm ơn thầy vì đã xem qua bài báo cáo trình bày của em. Vì đây là lần đầu tiên huấn luyện mô hình nên trong quá trình viết báo cáo và thực hành huấn luyện có thể có thiếu sót, hoặc có đôi chỗ em chưa nắm vững kiến thức, mong thầy thông cảm và bỏ qua.\n",
    "\n",
    "Chúc thầy một ngày tốt lành ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd7b9c",
   "metadata": {},
   "source": [
    "# 6. Tài liệu tham khảo\n",
    "1. Thông tin về môi trường huấn luyện Lunar Lander: [Lunar Lander Gymnasium Documentation](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "2. Kiến thức về Advantage Actor Critic (A2C) cho người mới: [Actor Critic methods with Robotics environments](https://huggingface.co/learn/deep-rl-course/unit6/introduction)\n",
    "3. Kiến thức về Proximal Policy Optimization (PPO) cho người mới: [Proximal Policy Optimization (PPO)](https://huggingface.co/learn/deep-rl-course/unit6/introduction)\n",
    "4. Kiến thức về Advantage Actor Critic (A2C) trên trang DataCamp: [Advantage Actor Critic](https://campus.datacamp.com/courses/deep-reinforcement-learning-in-python/introduction-to-policy-gradient-methods?ex=7)\n",
    "5. Kiến thức về Proximal Policy Optimization (PPO) trên trang DataCamp: [Proximal Policy Optimization](https://projector-video-pdf-converter.datacamp.com/36398/chapter4.pdf)\n",
    "6. Paper chính thức của A2C: [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783) \n",
    "7. Paper chính thức của PPO: [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)\n",
    "8. Paper chính thức của GAE: [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438)\n",
    "9. Dự án nghiên cứu Thạc sĩ: [Towards Delivering a Coherent Self-Contained Explanation of Proximal Policy Optimization](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8976480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded notebook saved as PPO_LunarLander_Embedded5.ipynb\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Paths to media\n",
    "image_dir = './images/'\n",
    "input_notebook = 'PPO_LunarLander.ipynb'\n",
    "output_notebook = 'PPO_LunarLander_Embedded5.ipynb'\n",
    "\n",
    "# Function to encode file to base64\n",
    "def file_to_base64(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        return base64.b64encode(file.read()).decode('utf-8')\n",
    "\n",
    "# Collect all image files\n",
    "image_files = [f'image{i}.png' for i in range(1, 52)]  # image1.png to image40.png\n",
    "\n",
    "# Load the notebook\n",
    "with open(input_notebook, 'r', encoding='utf-8') as f:\n",
    "    notebook = json.load(f)\n",
    "\n",
    "# Process each cell\n",
    "for cell in notebook['cells']:\n",
    "    if cell['cell_type'] == 'markdown':\n",
    "        source = ''.join(cell['source'])\n",
    "        # Replace image references\n",
    "        for img in image_files:\n",
    "            img_path = os.path.join(image_dir, img)\n",
    "            if os.path.exists(img_path):\n",
    "                img_b64 = file_to_base64(img_path)\n",
    "                img_pattern = rf'!\\[\\]\\(images/{img}\\)'\n",
    "                img_data_uri = f'![{img}](data:image/png;base64,{img_b64})'\n",
    "                source = re.sub(img_pattern, img_data_uri, source)\n",
    "        # Update cell source\n",
    "        cell['source'] = source.splitlines(keepends=True)\n",
    "\n",
    "# Save the new notebook\n",
    "with open(output_notebook, 'w', encoding='utf-8') as f:\n",
    "    json.dump(notebook, f, indent=2)\n",
    "\n",
    "print(f\"Embedded notebook saved as {output_notebook}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
