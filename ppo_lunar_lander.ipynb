{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb21b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,     \n",
    "    gravity=-10.0,        \n",
    "    enable_wind=False,   \n",
    "    wind_power=15.0,      \n",
    "    turbulence_power=1.0, \n",
    "    render_mode=\"rgb_array\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73defbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Training parameters (following paper)\n",
    "gamma = 0.99                # Discount factor\n",
    "lr = 3e-4                   # Learning rate\n",
    "lamb = 0.95                 # Generalised Advantage Estimation (GAE) lambda\n",
    "epsilon = 0.2               # Clipping value\n",
    "h = 0.01                    # Entropy coefficient\n",
    "v = 0.5                     # Value loss coefficient\n",
    "max_timesteps = 1e6         # Maximal number of iterations\n",
    "N = 1                       # Number of agents collecting training data\n",
    "T = 2048                    # Maximal trajectory length\n",
    "K = 10                      # Number of epoches per update\n",
    "minibatch_size = 64         # Size of a mini batch\n",
    "number_minibatches = N * T / minibatch_size     # Number of mini batches\n",
    "actor_losses = []\n",
    "critic_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8abc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network to select an action\n",
    "ActorNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "\n",
    "# The network to get value of a state\n",
    "CriticNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "# Optimizer using Adam Gradient Descent\n",
    "actor_optimizer = optim.Adam(ActorNetwork.parameters(), lr=lr)\n",
    "critic_optimizer = optim.Adam(CriticNetwork.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9547274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GAE estimates the advantage of taking an action in a state\n",
    "\"\"\"\n",
    "def compute_GAE(next_value, rewards, values, dones):\n",
    "    advantages = []\n",
    "    GAE = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]      # TD error\n",
    "        GAE = delta + gamma * lamb * (1 - dones[t]) * GAE\n",
    "        advantages.insert(0, GAE)\n",
    "        next_value = values[t]\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd4bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()      # Initialize state s_t\n",
    "timesteps = 0\n",
    "\n",
    "while timesteps < max_timesteps:\n",
    "    states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n",
    "\n",
    "    for _ in range(T):              # Collect T timesteps for 1 rollout\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        value = CriticNetwork(state_tensor)\n",
    "        action_pred = ActorNetwork(state_tensor)                       # Select action a_t\n",
    "        dist = distributions.Categorical(action_pred)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)        # Old policy given action a_t\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())      # Advance simulation one time step\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Collect training data\n",
    "        states.append(state)         # Collect states\n",
    "        actions.append(action)        # Collect actions from Actor Network\n",
    "        rewards.append(reward)        # Collect rewards\n",
    "        log_probs.append(log_prob)      # Collect lob_probs\n",
    "        values.append(value)         # Collect values from Critic Network\n",
    "        dones.append(done)          # Collect done (0 or 1)\n",
    "\n",
    "        state = next_state      # Move to next state\n",
    "        timesteps += 1          # Increase timesteps\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()      # If an episode is done, reset the state\n",
    "    \n",
    "    # Compute advantages\n",
    "    next_state_tensor = torch.FloatTensor(state)            # Get the last state reached after last action\n",
    "    next_value = CriticNetwork(next_state_tensor)           # Calculate the next value\n",
    "    advantages = compute_GAE(next_value, rewards=rewards, values=values, dones=dones)       # Calculate advantage using GAE\n",
    "    advantages = torch.FloatTensor(advantages)\n",
    "    returns = advantages + torch.FloatTensor(values)        # Calculate V-target_t = A_t + V-w(s_t)\n",
    "\n",
    "    # Optimize policy\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    old_log_probs = torch.FloatTensor(log_probs)\n",
    "    returns = returns.detach()\n",
    "\n",
    "    for k in range(K):\n",
    "        for i in torch.randperm(len(states)).split(minibatch_size):        # Sample over batches with size 'minibatch_size'      \n",
    "            batch_states = states[i]\n",
    "            batch_actions = actions[i]\n",
    "            batch_old_log_probs = old_log_probs[i]\n",
    "            batch_returns = returns[i]\n",
    "            batch_advantages = advantages[i]\n",
    "\n",
    "            optimized_value = CriticNetwork(batch_states)\n",
    "            optimized_action_preds = ActorNetwork(batch_states)\n",
    "            optimized_dist = distributions.Categorical(optimized_action_preds)\n",
    "            optimized_log_probs = optimized_dist.log_prob(batch_actions)\n",
    "        \n",
    "            ratio = (optimized_log_probs - batch_old_log_probs).exp()\n",
    "            unclipped_objective = ratio * batch_advantages\n",
    "            clipped_objective = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * batch_advantages\n",
    "\n",
    "            L_clip = -torch.min(unclipped_objective, clipped_objective).mean()\n",
    "            L_v = (optimized_value.squeeze() - batch_returns).pow(2).mean()\n",
    "            H = optimized_dist.entropy().mean()\n",
    "\n",
    "            # Loss = L_clip + v * L_v - h * H\n",
    "\n",
    "            # Actor loss\n",
    "            actor_loss = L_clip - h * H\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)  # Retain graph if value_loss uses same computation graph\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Critic loss\n",
    "            critic_optimizer.zero_grad()\n",
    "            L_v.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
