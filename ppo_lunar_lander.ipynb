{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bb21b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,     \n",
    "    gravity=-10.0,        \n",
    "    enable_wind=False,   \n",
    "    wind_power=15.0,      \n",
    "    turbulence_power=1.0, \n",
    "    render_mode=\"rgb_array\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f73defbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Training parameters (following paper)\n",
    "gamma = 0.99                # Discount factor\n",
    "lr = 3e-4                   # Learning rate\n",
    "lamb = 0.95                 # Generalised Advantage Estimation (GAE) lambda\n",
    "epsilon = 0.2               # Clipping value\n",
    "h = 0.01                    # Entropy coefficient\n",
    "v = 0.5                     # Value loss coefficient\n",
    "max_timesteps = 1e6         # Maximal number of iterations\n",
    "eval_epochs = 1000           # Epochs for evaluation\n",
    "N = 1                       # Number of agents collecting training data\n",
    "T = 2048                    # Maximal trajectory length\n",
    "K = 10                      # Number of epoches per update\n",
    "max_accuracy = 0            # Accuracy for evaluation\n",
    "max_reward = 0\n",
    "minibatch_size = 64         # Size of a mini batch\n",
    "number_minibatches = N * T / minibatch_size     # Number of mini batches\n",
    "actor_losses = []\n",
    "critic_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8abc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network to select an action\n",
    "ActorNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, action_dim)\n",
    ")\n",
    "\n",
    "# The network to get value of a state\n",
    "CriticNetwork = nn.Sequential(\n",
    "    nn.Linear(space_dim, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "# Optimizer using Adam Gradient Descent\n",
    "actor_optimizer = optim.Adam(ActorNetwork.parameters(), lr=lr)\n",
    "critic_optimizer = optim.Adam(CriticNetwork.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9547274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GAE estimates the advantage of taking an action in a state\n",
    "\"\"\"\n",
    "def compute_GAE(next_value, rewards, values, dones):\n",
    "    advantages = []\n",
    "    GAE = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]      # TD error\n",
    "        GAE = delta + gamma * lamb * (1 - dones[t]) * GAE\n",
    "        advantages.insert(0, GAE)\n",
    "        next_value = values[t].item()\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eed4d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,     \n",
    "    gravity=-10.0,        \n",
    "    enable_wind=False,   \n",
    "    wind_power=15.0,      \n",
    "    turbulence_power=1.0, \n",
    "    render_mode=\"rgb_array\" \n",
    ")\n",
    "\n",
    "def evaluation(eval_epochs):\n",
    "    total_reward = 0\n",
    "    for i in range(eval_epochs):\n",
    "        state, _ = video_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_pred = ActorNetwork(state_tensor)\n",
    "            action = torch.argmax(action_pred).item()\n",
    "            state, reward, terminated, truncated, _ = video_env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "    return total_reward / eval_epochs     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dd4bed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "In current timestep: 2048, average reward: -133.66161433331453\n",
      "2048\n",
      "In current timestep: 4096, average reward: -326.0962024724145\n",
      "4096\n",
      "In current timestep: 6144, average reward: -294.67870109642917\n",
      "6144\n",
      "In current timestep: 8192, average reward: -227.07719927348182\n",
      "8192\n",
      "In current timestep: 10240, average reward: -255.0342714625101\n",
      "10240\n",
      "In current timestep: 12288, average reward: -275.13861737629065\n",
      "12288\n",
      "In current timestep: 14336, average reward: -212.53248713509012\n",
      "14336\n",
      "In current timestep: 16384, average reward: -202.45216735494563\n",
      "16384\n",
      "In current timestep: 18432, average reward: -199.82398490698728\n",
      "18432\n",
      "In current timestep: 20480, average reward: -177.88119927457382\n",
      "20480\n",
      "In current timestep: 22528, average reward: -170.68942691179143\n",
      "22528\n",
      "In current timestep: 24576, average reward: -146.54267035096208\n",
      "24576\n",
      "In current timestep: 26624, average reward: -80.05114126714214\n",
      "26624\n",
      "In current timestep: 28672, average reward: -149.3803054471911\n",
      "28672\n",
      "In current timestep: 30720, average reward: -169.3051624315186\n",
      "30720\n",
      "In current timestep: 32768, average reward: -166.3267260728953\n",
      "32768\n",
      "In current timestep: 34816, average reward: -151.12124477430822\n",
      "34816\n",
      "In current timestep: 36864, average reward: -188.0777109594316\n",
      "36864\n",
      "In current timestep: 38912, average reward: -221.73731266596639\n",
      "38912\n",
      "In current timestep: 40960, average reward: -196.7567411139583\n",
      "40960\n",
      "In current timestep: 43008, average reward: -232.5859821501609\n",
      "43008\n",
      "In current timestep: 45056, average reward: -248.7651353565833\n",
      "45056\n",
      "In current timestep: 47104, average reward: -220.43363825434682\n",
      "47104\n",
      "In current timestep: 49152, average reward: -171.90246096347505\n",
      "49152\n",
      "In current timestep: 51200, average reward: -163.49024819093336\n",
      "51200\n",
      "In current timestep: 53248, average reward: -62.215346462741635\n",
      "53248\n",
      "In current timestep: 55296, average reward: -23.384921690325164\n",
      "55296\n",
      "In current timestep: 57344, average reward: 118.80484919910248\n",
      "57344\n",
      "In current timestep: 59392, average reward: 129.65867304288403\n",
      "59392\n",
      "In current timestep: 61440, average reward: 119.03757075116681\n",
      "61440\n",
      "In current timestep: 63488, average reward: 128.3497787217365\n",
      "63488\n",
      "In current timestep: 65536, average reward: 182.79277299923143\n",
      "65536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timesteps % \u001b[32m2048\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         avg_rewards = \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIn current timestep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, average reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_rewards\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mevaluation\u001b[39m\u001b[34m(eval_epochs)\u001b[39m\n\u001b[32m     18\u001b[39m action_pred = ActorNetwork(state_tensor)\n\u001b[32m     19\u001b[39m action = torch.argmax(action_pred).item()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m state, reward, terminated, truncated, _ = \u001b[43mvideo_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m total_reward += reward\n\u001b[32m     22\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gymer-env/lib/python3.13/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gymer-env/lib/python3.13/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gymer-env/lib/python3.13/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gymer-env/lib/python3.13/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gymer-env/lib/python3.13/site-packages/gymnasium/envs/box2d/lunar_lander.py:621\u001b[39m, in \u001b[36mLunarLander.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    607\u001b[39m         p.ApplyLinearImpulse(\n\u001b[32m    608\u001b[39m             (\n\u001b[32m    609\u001b[39m                 ox * SIDE_ENGINE_POWER * s_power,\n\u001b[32m   (...)\u001b[39m\u001b[32m    613\u001b[39m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    614\u001b[39m         )\n\u001b[32m    615\u001b[39m     \u001b[38;5;28mself\u001b[39m.lander.ApplyLinearImpulse(\n\u001b[32m    616\u001b[39m         (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n\u001b[32m    617\u001b[39m         impulse_pos,\n\u001b[32m    618\u001b[39m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    619\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m pos = \u001b[38;5;28mself\u001b[39m.lander.position\n\u001b[32m    624\u001b[39m vel = \u001b[38;5;28mself\u001b[39m.lander.linearVelocity\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gymer-env/lib/python3.13/site-packages/gymnasium/envs/box2d/lunar_lander.py:64\u001b[39m, in \u001b[36mContactDetector.BeginContact\u001b[39m\u001b[34m(self, contact)\u001b[39m\n\u001b[32m     61\u001b[39m     contactListener.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m.env = env\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mBeginContact\u001b[39m(\u001b[38;5;28mself\u001b[39m, contact):\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     66\u001b[39m         \u001b[38;5;28mself\u001b[39m.env.lander == contact.fixtureA.body\n\u001b[32m     67\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.lander == contact.fixtureB.body\n\u001b[32m     68\u001b[39m     ):\n\u001b[32m     69\u001b[39m         \u001b[38;5;28mself\u001b[39m.env.game_over = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()      # Initialize state s_t\n",
    "timesteps = 0\n",
    "\n",
    "while timesteps < max_timesteps:\n",
    "    states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n",
    "    print(timesteps)\n",
    "    for _ in range(T):              # Collect T timesteps for 1 rollout\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        value = CriticNetwork(state_tensor)\n",
    "        action_pred = ActorNetwork(state_tensor)                       # Select action a_t\n",
    "        dist = distributions.Categorical(logits=action_pred)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)        # Old policy given action a_t\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())      # Advance simulation one time step\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Collect training data\n",
    "        states.append(state)         # Collect states\n",
    "        actions.append(action)        # Collect actions from Actor Network\n",
    "        rewards.append(reward)        # Collect rewards\n",
    "        log_probs.append(log_prob)      # Collect lob_probs\n",
    "        values.append(value)         # Collect values from Critic Network\n",
    "        dones.append(done)          # Collect done (0 or 1)\n",
    "\n",
    "        state = next_state      # Move to next state\n",
    "        timesteps += 1          # Increase timesteps\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()      # If an episode is done, reset the state\n",
    "    \n",
    "    # Compute advantages\n",
    "    next_state_tensor = torch.FloatTensor(state)            # Get the last state reached after last action\n",
    "    next_value = CriticNetwork(next_state_tensor)           # Calculate the next value\n",
    "    advantages = compute_GAE(next_value, rewards=rewards, values=values, dones=dones)       # Calculate advantage using GAE\n",
    "    advantages = torch.FloatTensor(advantages)\n",
    "    returns = advantages + torch.FloatTensor(values)        # Calculate V-target_t = A_t + V-w(s_t)\n",
    "\n",
    "    # Optimize policy\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    old_log_probs = torch.FloatTensor(log_probs)\n",
    "    returns = returns.detach()\n",
    "\n",
    "    for k in range(K):\n",
    "        for i in torch.randperm(len(states)).split(minibatch_size):        # Sample over batches with size 'minibatch_size'      \n",
    "            batch_states = states[i]\n",
    "            batch_actions = actions[i]\n",
    "            batch_old_log_probs = old_log_probs[i]\n",
    "            batch_returns = returns[i]\n",
    "            batch_advantages = advantages[i]\n",
    "\n",
    "            optimized_value = CriticNetwork(batch_states)\n",
    "            optimized_action_preds = ActorNetwork(batch_states)\n",
    "            optimized_dist = distributions.Categorical(logits=optimized_action_preds)\n",
    "            optimized_log_probs = optimized_dist.log_prob(batch_actions)\n",
    "        \n",
    "            ratio = (optimized_log_probs - batch_old_log_probs).exp()\n",
    "            unclipped_objective = ratio * batch_advantages\n",
    "            clipped_objective = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * batch_advantages\n",
    "\n",
    "            L_clip = -torch.min(unclipped_objective, clipped_objective).mean()\n",
    "            L_v = (optimized_value.squeeze() - batch_returns).pow(2).mean()\n",
    "            H = optimized_dist.entropy().mean()\n",
    "\n",
    "            # Loss = L_clip + v * L_v - h * H\n",
    "\n",
    "            # Actor loss\n",
    "            actor_loss = L_clip - h * H\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Critic loss\n",
    "            critic_optimizer.zero_grad()\n",
    "            L_v.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "    if timesteps % 2048 == 0:\n",
    "        try:\n",
    "            avg_rewards = evaluation(100)\n",
    "            print(f\"In current timestep: {timesteps}, average reward: {avg_rewards}\", flush=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluation: {e}\", flush=True)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
